# Fix Implementation Plan: E2E Test Failures

**Goal:** Achieve 75-90% test pass rate (28-33 passing tests out of 37 active tests)

**Current State:** 20/37 passing (54%)
**Target State:** 28-33/37 passing (75-90%)
**Estimated Total Effort:** 4 hours

---

## Fix Priority Matrix

| Fix | Tests Fixed | Effort | Impact | Priority |
|-----|-------------|--------|--------|----------|
| 1. Project Code Reviews API | 2 tests | 2 hours | High | ðŸ”´ P0 |
| 2. Update Checkpoint Validation Test | 1 test | 15 mins | Medium | ðŸŸ¡ P1 |
| 3. Update WebSocket Test | 1 test | 30 mins | Medium | ðŸŸ¡ P1 |
| 4. Investigation Buffer | TBD | 1.25 hours | Unknown | ðŸŸ¢ P2 |

---

## ðŸ”´ P0: Fix #1 - Add Project Code Reviews API

**Impact:** Fixes 2 tests immediately (22/37 passing = 59%)
**Effort:** 2 hours
**Difficulty:** Medium

### Implementation Steps

#### Step 1.1: Create Backend API Endpoint (45 mins)

**File:** `/home/frankbria/projects/codeframe/codeframe/api/routes.py`

```python
# Add new endpoint
@app.get("/api/projects/{project_id}/code-reviews")
async def get_project_code_reviews(
    project_id: int,
    db: Database = Depends(get_database)
) -> ReviewResult:
    """
    Get aggregated code review results for a project.

    Returns summary statistics:
    - Total findings count
    - Breakdown by severity (critical, high, medium, low, info)
    - Breakdown by category (security, performance, quality, etc.)
    - Blocking status (true if critical/high findings exist)
    """
    try:
        # Fetch all code reviews for project
        reviews = db.get_code_reviews_by_project(project_id)

        # Calculate severity counts
        severity_counts = {
            'critical': 0,
            'high': 0,
            'medium': 0,
            'low': 0,
            'info': 0
        }
        for review in reviews:
            severity_counts[review.severity] += 1

        # Calculate category counts
        category_counts = {
            'security': 0,
            'performance': 0,
            'quality': 0,
            'maintainability': 0,
            'style': 0
        }
        for review in reviews:
            category_counts[review.category] += 1

        # Determine blocking status
        has_blocking_findings = (
            severity_counts['critical'] > 0 or
            severity_counts['high'] > 0
        )

        return ReviewResult(
            total_count=len(reviews),
            severity_counts=severity_counts,
            category_counts=category_counts,
            has_blocking_findings=has_blocking_findings
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**Validation:**
```bash
# Test endpoint
curl -s http://localhost:8080/api/projects/2/code-reviews | jq .

# Expected output:
# {
#   "total_count": 7,
#   "severity_counts": {"critical": 1, "high": 2, "medium": 3, "low": 1, "info": 0},
#   "category_counts": {"security": 1, "performance": 0, "quality": 1, "maintainability": 3, "style": 2},
#   "has_blocking_findings": true
# }
```

#### Step 1.2: Add Database Method (30 mins)

**File:** `/home/frankbria/projects/codeframe/codeframe/persistence/database.py`

```python
def get_code_reviews_by_project(self, project_id: int) -> list[CodeReview]:
    """
    Get all code review findings for a project.

    Args:
        project_id: Project ID to fetch reviews for

    Returns:
        List of CodeReview objects
    """
    cursor = self.conn.cursor()
    cursor.execute(
        """
        SELECT id, project_id, agent_id, task_id, file_path, line_number,
               severity, category, message, recommendation, code_snippet, created_at
        FROM code_reviews
        WHERE project_id = ?
        ORDER BY severity DESC, created_at DESC
        """,
        (project_id,)
    )

    rows = cursor.fetchall()
    return [
        CodeReview(
            id=row[0],
            project_id=row[1],
            agent_id=row[2],
            task_id=row[3],
            file_path=row[4],
            line_number=row[5],
            severity=row[6],
            category=row[7],
            message=row[8],
            recommendation=row[9],
            code_snippet=row[10],
            created_at=row[11]
        )
        for row in rows
    ]
```

**Validation:**
```python
# Test in Python REPL
from codeframe.persistence.database import Database
db = Database("tests/e2e/.codeframe/state.db")
reviews = db.get_code_reviews_by_project(2)
print(f"Found {len(reviews)} reviews")
for r in reviews[:3]:
    print(f"  {r.severity} - {r.file_path}:{r.line_number} - {r.message}")
```

#### Step 1.3: Add Frontend API Client (15 mins)

**File:** `/home/frankbria/projects/codeframe/web-ui/src/api/reviews.ts`

```typescript
/**
 * Get aggregated review results for a project
 *
 * @param projectId - Project ID to fetch reviews for
 * @returns Promise resolving to ReviewResult
 * @throws Error if request fails
 */
export async function getProjectReviews(
  projectId: number
): Promise<ReviewResult> {
  const response = await fetch(
    `${API_BASE_URL}/api/projects/${projectId}/code-reviews`,
    {
      method: 'GET',
      headers: {
        'Content-Type': 'application/json',
      },
    }
  );

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(
      `Failed to fetch project reviews: ${response.status} ${errorText}`
    );
  }

  return response.json();
}
```

#### Step 1.4: Update Dashboard Component (30 mins)

**File:** `/home/frankbria/projects/codeframe/web-ui/src/components/Dashboard.tsx`

```typescript
import { getProjectReviews } from '@/api/reviews';
import type { ReviewResult } from '@/types/reviews';

// Add state for review data
const [reviewData, setReviewData] = useState<ReviewResult | null>(null);
const [reviewLoading, setReviewLoading] = useState(true);
const [reviewError, setReviewError] = useState<string | null>(null);

// Fetch review data on mount
useEffect(() => {
  async function fetchReviews() {
    try {
      setReviewLoading(true);
      const data = await getProjectReviews(projectId);
      setReviewData(data);
      setReviewError(null);
    } catch (err) {
      setReviewError(err instanceof Error ? err.message : 'Failed to fetch reviews');
      setReviewData(null);
    } finally {
      setReviewLoading(false);
    }
  }

  fetchReviews();

  // Optional: Poll for updates every 30 seconds
  const intervalId = setInterval(fetchReviews, 30000);
  return () => clearInterval(intervalId);
}, [projectId]);

// Update ReviewSummary component usage
<ReviewSummary
  reviewResult={reviewData}
  loading={reviewLoading}
  error={reviewError}
/>
```

**Validation:**
```bash
# 1. Start dev server
cd web-ui && npm run dev

# 2. Open browser to http://localhost:3000/projects/2

# 3. Check console for API call
# Expected: GET http://localhost:8080/api/projects/2/code-reviews â†’ 200 OK

# 4. Verify UI shows review data
# Expected: "Total Findings: 7", severity breakdown chart visible
```

### Testing & Validation

```bash
# Run affected tests
npx playwright test test_dashboard.spec.ts:51 --project=chromium
npx playwright test test_review_ui.spec.ts:29 --project=chromium

# Expected output:
# Running 2 tests using 1 worker
#   âœ“ [chromium] â€º test_dashboard.spec.ts:51 - should display review findings panel (2.5s)
#   âœ“ [chromium] â€º test_review_ui.spec.ts:29 - should display review findings panel (2.3s)
#
# 2 passed (5s)
```

---

## ðŸŸ¡ P1: Fix #2 - Update Checkpoint Validation Test

**Impact:** Fixes 1 test (23/37 passing = 62%)
**Effort:** 15 minutes
**Difficulty:** Easy

### Implementation Steps

#### Step 2.1: Update Test Logic (10 mins)

**File:** `/home/frankbria/projects/codeframe/tests/e2e/test_checkpoint_ui.spec.ts`

**Before (Lines 76-96):**
```typescript
test('should validate checkpoint name input', async ({ page }) => {
  const createButton = page.locator('[data-testid="create-checkpoint-button"]');
  await createButton.click();

  const modal = page.locator('[data-testid="create-checkpoint-modal"]');
  const nameInput = modal.locator('[data-testid="checkpoint-name-input"]');
  const saveButton = modal.locator('[data-testid="checkpoint-save-button"]');

  // Try to save without name
  await saveButton.click(); // âŒ FAILS: Button is disabled

  // Validation error should appear
  const error = modal.locator('[data-testid="checkpoint-name-error"]');
  await expect(error).toBeVisible();

  // Enter valid name
  await nameInput.fill('Test Checkpoint');

  // Error should disappear
  await expect(error).not.toBeVisible();
});
```

**After (Updated to match component behavior):**
```typescript
test('should validate checkpoint name input', async ({ page }) => {
  const createButton = page.locator('[data-testid="create-checkpoint-button"]');
  await createButton.click();

  const modal = page.locator('[data-testid="create-checkpoint-modal"]');
  const nameInput = modal.locator('[data-testid="checkpoint-name-input"]');
  const saveButton = modal.locator('[data-testid="checkpoint-save-button"]');

  // Initially, save button should be disabled (no name entered)
  await expect(saveButton).toBeDisabled();

  // Try to interact with disabled button (should fail gracefully)
  // Note: Playwright won't click disabled buttons, which is correct behavior

  // Enter valid name
  await nameInput.fill('Test Checkpoint');

  // Button should now be enabled
  await expect(saveButton).toBeEnabled();

  // Clear name
  await nameInput.clear();

  // Button should be disabled again
  await expect(saveButton).toBeDisabled();
});
```

#### Step 2.2: Add Test Documentation (5 mins)

**Add comment explaining validation strategy:**
```typescript
/**
 * Checkpoint name validation test.
 *
 * Component Design: Client-side validation via disabled button
 * - Save button is disabled when name is empty
 * - No error message shown (UX relies on disabled state)
 * - This prevents invalid form submission
 *
 * Test Strategy: Verify disabled state changes with input
 * - Empty name â†’ disabled button
 * - Valid name â†’ enabled button
 */
test('should validate checkpoint name input', async ({ page }) => {
  // ...
});
```

### Testing & Validation

```bash
# Run test
npx playwright test test_checkpoint_ui.spec.ts:76 --project=chromium
npx playwright test test_checkpoint_ui.spec.ts:76 --project=firefox

# Expected output:
# Running 2 tests using 2 workers
#   âœ“ [chromium] â€º test_checkpoint_ui.spec.ts:76 - should validate checkpoint name input (1.2s)
#   âœ“ [firefox] â€º test_checkpoint_ui.spec.ts:76 - should validate checkpoint name input (1.3s)
#
# 2 passed (2.5s)
```

---

## ðŸŸ¡ P1: Fix #3 - Update WebSocket Test

**Impact:** Fixes 1 test (24/37 passing = 65%)
**Effort:** 30 minutes
**Difficulty:** Medium

### Implementation Steps

#### Step 3.1: Investigate Current Architecture (10 mins)

```bash
# Check if WebSocket server is implemented
grep -r "websocket\|WebSocket" codeframe/api/ --include="*.py"

# Check AgentStateProvider implementation
grep -A 20 "class.*AgentStateProvider\|function AgentStateProvider" web-ui/src/components/AgentStateProvider.tsx
```

**Decision Tree:**
- **If WebSocket server exists:** Update test to check for connection
- **If WebSocket server doesn't exist:** Skip test until feature is implemented

#### Step 3.2: Option A - Update Test (If WS Exists)

**File:** `/home/frankbria/projects/codeframe/tests/e2e/test_dashboard.spec.ts`

```typescript
test('should receive real-time updates via WebSocket', async ({ page }) => {
  // Monitor network for WebSocket connection
  const wsPromise = page.waitForEvent('websocket', { timeout: 10000 });

  // Navigate to dashboard (triggers AgentStateProvider connection)
  await page.goto(`${FRONTEND_URL}/projects/${PROJECT_ID}`);
  await page.waitForLoadState('networkidle');

  // WebSocket should auto-connect via AgentStateProvider
  const ws = await wsPromise;

  // Verify connection URL
  expect(ws.url()).toContain('/ws');

  // Send mock message from server (simulate agent state update)
  // Note: This requires mocking server-side WebSocket messages
  // which may not be possible in Playwright without backend changes

  // Alternative: Just verify connection established
  expect(ws.isClosed()).toBe(false);
});
```

#### Step 3.3: Option B - Skip Test (If WS Not Implemented)

```typescript
test.skip('should receive real-time updates via WebSocket', async ({ page }) => {
  // SKIP: WebSocket server not yet implemented in backend
  // This feature is planned for future sprint
  // Related: AgentStateProvider exists but backend WS endpoint missing

  // TODO: Implement backend WebSocket endpoint at /ws
  // TODO: Update test when feature is complete
});
```

### Testing & Validation

```bash
# Run test
npx playwright test test_dashboard.spec.ts:134 --project=chromium

# Expected output (if updated):
# âœ“ [chromium] â€º test_dashboard.spec.ts:134 - should receive real-time updates via WebSocket (3.5s)

# OR (if skipped):
# â—‹ [chromium] â€º test_dashboard.spec.ts:134 - should receive real-time updates via WebSocket (skipped)
```

---

## ðŸŸ¢ P2: Investigation Buffer (1.25 hours)

**Purpose:** Handle unexpected issues during implementation

### Tasks

1. **Regression Testing (30 mins)**
   - Run full test suite after each fix
   - Ensure no new failures introduced
   - Document any new issues

2. **Fix Validation (20 mins)**
   - Verify all fixes work across browsers (Chromium, Firefox, WebKit)
   - Check mobile viewports (Mobile Chrome, Mobile Safari)
   - Screenshot visual differences

3. **Code Review Self-Check (15 mins)**
   - Review all code changes for quality
   - Check TypeScript type safety
   - Verify error handling

4. **Documentation Update (20 mins)**
   - Update `PHASE2_TEST_ANALYSIS.md` with new results
   - Create `PHASE2C_COMPLETION_REPORT.md`
   - Update `E2E_TEST_DATA_REQUIREMENTS.md` if needed

---

## Execution Timeline

### Day 1 (4 hours)

**Morning Session (2 hours)**
- 09:00-09:45: Implement backend API endpoint (Step 1.1)
- 09:45-10:15: Add database method (Step 1.2)
- 10:15-10:30: Add frontend API client (Step 1.3)
- 10:30-11:00: Update Dashboard component (Step 1.4)

**Afternoon Session (2 hours)**
- 13:00-13:30: Test & validate Fix #1 (review API)
- 13:30-13:45: Implement Fix #2 (checkpoint test)
- 13:45-14:00: Test & validate Fix #2
- 14:00-14:30: Implement Fix #3 (WebSocket test)
- 14:30-15:00: Investigation buffer & regression testing
- 15:00-16:00: Final validation & documentation

---

## Success Criteria

### Minimum Success (75% pass rate)
- âœ… 28/37 tests passing (75%)
- âœ… 0 tests failing (all skipped tests are intentional)
- âœ… All fixes validated across 3 browsers

### Target Success (90% pass rate)
- âœ… 33/37 tests passing (90%)
- âœ… 0 tests failing
- âœ… All documentation updated

### Stretch Goal (100% implementation)
- âœ… 37/37 tests passing (100%)
- âœ… All skipped tests implemented
- âœ… CI/CD pipeline green

---

## Rollback Plan

**If Fix #1 Breaks Things:**
```bash
# Revert backend changes
git checkout HEAD -- codeframe/api/routes.py
git checkout HEAD -- codeframe/persistence/database.py

# Revert frontend changes
git checkout HEAD -- web-ui/src/components/Dashboard.tsx
git checkout HEAD -- web-ui/src/api/reviews.ts

# Re-run tests
npm test
```

**If Fix #2 or #3 Breaks Tests:**
```bash
# Revert test file
git checkout HEAD -- tests/e2e/test_checkpoint_ui.spec.ts
# OR
git checkout HEAD -- tests/e2e/test_dashboard.spec.ts

# Re-run tests
npm test
```

---

## Post-Implementation Checklist

- [ ] All 3 fixes implemented
- [ ] 24+ tests passing (65%+)
- [ ] No new test failures introduced
- [ ] Code reviewed for quality
- [ ] Documentation updated
- [ ] Git commits created with clear messages
- [ ] Test artifacts cleaned up (`test-results/` reviewed)
- [ ] `PHASE2C_COMPLETION_REPORT.md` created
- [ ] Next steps identified for Phase 3

---

## Next Steps (After Phase 2c)

**Phase 3 Goals:**
1. Implement skipped features (quality gates, review filtering, etc.)
2. Achieve 90-100% test pass rate
3. Add CI/CD integration (GitHub Actions)
4. Performance testing & optimization

**Estimated Effort:** 8-12 hours over 2-3 days
